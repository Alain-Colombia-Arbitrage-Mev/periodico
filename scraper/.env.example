# ========================================
# Supabase Configuration
# ========================================
# Obtén estas credenciales en: https://supabase.com/dashboard/project/YOUR_PROJECT/settings/api

# URL de tu proyecto Supabase
SUPABASE_URL=https://your-project.supabase.co

# Anon/Public Key (para queries de base de datos)
# En Dashboard: Project Settings > API > anon public
SUPABASE_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...

# Service Role Key (REQUERIDA para subir imágenes a Storage)
# En Dashboard: Project Settings > API > service_role
# ⚠️ MANTÉN ESTA KEY SECRETA - Tiene permisos de administrador
SUPABASE_SERVICE_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...

# ========================================
# OpenRouter API (for LLM rewriting)
# ========================================
# Obtén tu API key en: https://openrouter.ai/keys
OPENROUTER_API_KEY=sk-or-v1-your-key-here

# ========================================
# LLM Configuration
# ========================================
# Modelos recomendados (en orden de costo):
# - deepseek/deepseek-v3.2-exp     (RECOMENDADO - Mejor relación calidad/precio)
# - deepseek/deepseek-chat         (Económico)
# - anthropic/claude-3.5-sonnet    (Premium - Mejor calidad)
# - anthropic/claude-3-opus        (Premium Plus)
# Ver más en: https://openrouter.ai/models
LLM_MODEL=deepseek/deepseek-v3.2-exp
LLM_REWRITE_ENABLED=true

# ========================================
# Scraper Configuration
# ========================================
# Intervalo entre scraping (en segundos)
# 3600 = 1 hora, 21600 = 6 horas
SCRAPE_INTERVAL=3600

# Máximo de artículos por categoría
MAX_ARTICLES_PER_CATEGORY=20

# ========================================
# Image Configuration
# ========================================
DOWNLOAD_IMAGES=true
IMAGE_QUALITY=85
MAX_IMAGE_SIZE=2048

# ========================================
# Logging
# ========================================
LOG_LEVEL=INFO

# ========================================
# Run Mode
# ========================================
# once = ejecutar una sola vez y salir
# continuous = ejecutar continuamente cada SCRAPE_INTERVAL segundos
RUN_MODE=continuous
